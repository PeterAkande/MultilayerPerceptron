# MultilayerPerceptron
Just Got up and Decided to clear the drought of repositories on my GitHub page.
It Works on the principle of Back Propagation (All MLPs Do ðŸ˜‚), the ouput layer is Made up of Stacked Sigmoids...Lazy to adapt it to Softmax ðŸ˜ª.
The network is initialized with the training set and the accompanying labels,A list, whose length connotes the amount of layers and the value of each layer index in the list denotes the amount of nodes that particular  layer should have,a one_hot parameter ,that is by  default  set  to false...But can be Set to True if labels are not one_hot_encoded.
The first layer of the list to be passed should be the Number of features and the last oyem of the list shpuld be the amount of classes.


Would write a blog post on how MLPs work soon...
